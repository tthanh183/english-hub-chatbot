{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11740124,"sourceType":"datasetVersion","datasetId":7370061}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets accelerate -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T09:50:41.884432Z","iopub.execute_input":"2025-05-09T09:50:41.885218Z","iopub.status.idle":"2025-05-09T09:50:45.440412Z","shell.execute_reply.started":"2025-05-09T09:50:41.885194Z","shell.execute_reply":"2025-05-09T09:50:45.439110Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"import shutil\nshutil.rmtree(\"/kaggle/working/results\", ignore_errors=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T09:50:45.442494Z","iopub.execute_input":"2025-05-09T09:50:45.442807Z","iopub.status.idle":"2025-05-09T09:50:45.446933Z","shell.execute_reply.started":"2025-05-09T09:50:45.442776Z","shell.execute_reply":"2025-05-09T09:50:45.446343Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"import pandas as pd\nimport json\n\ndf = pd.read_csv(\"/kaggle/input/output/output.csv\")  # Đổi path nếu cần\n\ndefinition_templates = [\n    \"What does {word} mean?\",\n    \"Can you define {word}?\",\n    \"Tell me the meaning of {word}.\",\n    \"What's the meaning of {word}?\",\n    \"Explain the word {word}.\"\n]\nexample_templates = [\n    \"Can you give me an example of {word}?\",\n    \"Show me a sentence using {word}.\",\n    \"How is {word} used in a sentence?\",\n    \"Give an example sentence with {word}.\",\n    \"Use {word} in a sentence.\"\n]\n\ndata = []\nfor _, row in df.iterrows():\n    word = str(row[\"word\"])\n    definition = str(row[\"definition\"]) if pd.notnull(row[\"definition\"]) else \"\"\n    example = str(row[\"example\"]) if pd.notnull(row[\"example\"]) else \"\"\n    for temp in definition_templates:\n        instruction = temp.format(word=word)\n        response = f\"{word.capitalize()} means {definition}.\"\n        data.append({\"instruction\": instruction, \"response\": response})\n    for temp in example_templates:\n        instruction = temp.format(word=word)\n        response = example\n        data.append({\"instruction\": instruction, \"response\": response})\n\nwith open(\"vocab_data.jsonl\", \"w\", encoding=\"utf-8\") as f:\n    for item in data:\n        json.dump(item, f, ensure_ascii=False)\n        f.write(\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T09:50:45.447791Z","iopub.execute_input":"2025-05-09T09:50:45.448082Z","iopub.status.idle":"2025-05-09T09:50:45.929802Z","shell.execute_reply.started":"2025-05-09T09:50:45.448059Z","shell.execute_reply":"2025-05-09T09:50:45.928861Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\nimport torch\n\ntorch.cuda.empty_cache()\ndataset = load_dataset('json', data_files={'train': 'vocab_data.jsonl'})\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\nmodel.resize_token_embeddings(len(tokenizer))\n\ndef preprocess_function(examples):\n    input_ids_list = []\n    labels_list = []\n    max_length = 128\n    for prompt, response in zip(examples['instruction'], examples['response']):\n        prompt_ids = tokenizer(prompt, truncation=True, max_length=64, add_special_tokens=False)['input_ids']\n        response_ids = tokenizer(response, truncation=True, max_length=64, add_special_tokens=False)['input_ids']\n        input_ids = prompt_ids + [tokenizer.eos_token_id] + response_ids + [tokenizer.eos_token_id]\n        labels = [-100] * (len(prompt_ids) + 1) + response_ids + [tokenizer.eos_token_id]\n        input_ids = input_ids[:max_length] + [tokenizer.pad_token_id] * (max_length - len(input_ids))\n        labels = labels[:max_length] + [-100] * (max_length - len(labels))\n        input_ids_list.append(input_ids)\n        labels_list.append(labels)\n    return {\n        'input_ids': input_ids_list,\n        'labels': labels_list,\n        'attention_mask': [[1 if id != tokenizer.pad_token_id else 0 for id in ids] for ids in input_ids_list]\n    }\n\ntrain_dataset = dataset['train'].map(preprocess_function, batched=True, remove_columns=dataset['train'].column_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T09:50:45.931780Z","iopub.execute_input":"2025-05-09T09:50:45.932025Z","iopub.status.idle":"2025-05-09T09:51:11.726975Z","shell.execute_reply.started":"2025-05-09T09:50:45.932009Z","shell.execute_reply":"2025-05-09T09:51:11.726123Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c0853c8c03c47198932d70306b6c8e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/30000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64cfdfe33fc5460d9af7726d7d78242b"}},"metadata":{}}],"execution_count":59},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./final_model\",\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=8,\n    num_train_epochs=5,\n    learning_rate=5e-5,\n    fp16=True,\n    save_strategy=\"no\",\n    logging_strategy=\"no\",\n    report_to=\"none\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    tokenizer=tokenizer\n)\n\ntrainer.train()\n\nmodel.save_pretrained(\"./final_model\")\ntokenizer.save_pretrained(\"./final_model\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T09:51:11.727941Z","iopub.execute_input":"2025-05-09T09:51:11.728731Z","iopub.status.idle":"2025-05-09T10:40:55.046063Z","shell.execute_reply.started":"2025-05-09T09:51:11.728709Z","shell.execute_reply":"2025-05-09T10:40:55.045127Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/2104615578.py:13: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2340' max='2340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2340/2340 49:39, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"('./final_model/tokenizer_config.json',\n './final_model/special_tokens_map.json',\n './final_model/vocab.json',\n './final_model/merges.txt',\n './final_model/added_tokens.json',\n './final_model/tokenizer.json')"},"metadata":{}}],"execution_count":60},{"cell_type":"code","source":"# 4. Sử dụng mô hình để hỏi đáp\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel = AutoModelForCausalLM.from_pretrained(\"./final_model\")\ntokenizer = AutoTokenizer.from_pretrained(\"./final_model\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\n\ndef ask_model(prompt):\n    formatted = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n    outputs = model.generate(**inputs, max_new_tokens=100, do_sample=False, temperature=0.7)\n    print(tokenizer.decode(outputs[0], skip_special_tokens=True).replace(formatted, \"\"))\n\n\n# Ví dụ\nask_model(\"What is the meaning of generation\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T11:33:49.903792Z","iopub.execute_input":"2025-05-09T11:33:49.904564Z","iopub.status.idle":"2025-05-09T11:33:50.488477Z","shell.execute_reply.started":"2025-05-09T11:33:49.904540Z","shell.execute_reply":"2025-05-09T11:33:50.487692Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nGeneration means A group of people born and living in the same year..\n","output_type":"stream"}],"execution_count":90}]}